# **The Dual Nature of Large Language Models: Helpers or Misleading Assistants**

This repository contains resources and materials related to an assignment based on the study *"Large Language Models as Misleading Assistants in Conversation"*. The analysis, visualizations, and conclusions in this assignment aim to build upon the insights provided in the research paper.

## **Resources**

- **Research Paper**: [arXiv](https://arxiv.org/pdf/2407.11789)  
  This assignment is based on the findings and methodologies presented in this foundational study.

- **Medium Article**: [Medium](https://medium.com/@sarvesh.borkar/the-dual-nature-of-large-language-models-helpers-or-misleading-assistants-8fc9fac0aa57)  
  A detailed write-up summarizing the assignment, including insights, methodology, and technical discussions.

- **Slideshare Presentation**: [Slideshare](https://www.slideshare.net/secret/b9BAMgOKo0JI29)  

- **Google Slides**: [Google Slides](https://docs.google.com/presentation/d/1ZV2TQ0fmJ90QuyVRyE97PXV00ILyCkygZWhO0xSiYu4/edit?usp=sharing)  

## **Overview**

The assignment examines:
- The risks of misleading outputs from LLMs.
- Experimental methodologies to evaluate deceptive behaviors in LLM-assisted interactions.
- Observations on the role of context in mitigating deceptive influences.

## **Acknowledgment**

This work is a study of the research paper *"Large Language Models as Misleading Assistants in Conversation"* by *Betty Li Hou, Kejian Shi, Jason Phang, James Aung, Steven Adler, and Rosie Campbell**. All credit for the original study and its findings goes to the authors. This assignment builds upon their insights to further understand and analyze the implications of their work.
