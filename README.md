# **The Dual Nature of Large Language Models: Helpers or Misleading Assistants**

This repository contains resources and materials related to an assignment based on the study *"Large Language Models as Misleading Assistants in Conversation"* by Betty Li Hou et al. The analysis, visualizations, and conclusions in this assignment aim to build upon the insights provided in the research paper.

## **Resources**

- **Research Paper**: [Read the full research paper on arXiv](https://arxiv.org/pdf/2407.11789)  
  This assignment is based on the findings and methodologies presented in this foundational study.

- **Medium Article**: [Read the full article on Medium](https://medium.com/@sarvesh.borkar/the-dual-nature-of-large-language-models-helpers-or-misleading-assistants-8fc9fac0aa57)  
  A detailed write-up summarizing the assignment, including insights, methodology, and technical discussions.

- **Slideshare Presentation**: [View the presentation on Slideshare](https://www.slideshare.net/secret/b9BAMgOKo0JI29)  

- **Google Slides**: [View the presentation on Google Slides](https://docs.google.com/presentation/d/1ZV2TQ0fmJ90QuyVRyE97PXV00ILyCkygZWhO0xSiYu4/edit?usp=sharing)  

## **Visualizations**

This repository includes:
- Accuracy comparison graphs for GPT-3.5 and GPT-4 under different configurations.
- Diagrams illustrating the experimental setup and metrics.

## **Overview**

The assignment examines:
- The risks of misleading outputs from LLMs.
- Experimental methodologies to evaluate deceptive behaviors in LLM-assisted interactions.
- Observations on the role of context in mitigating deceptive influences.

## **Acknowledgment**

This work is a study of the research paper *"Large Language Models as Misleading Assistants in Conversation"* by Betty Li Hou et al. All credit for the original study and its findings goes to the authors. This assignment builds upon their insights to further understand and analyze the implications of their work.
